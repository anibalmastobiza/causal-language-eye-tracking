<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eye Tracking Experiment</title>
    <script src="https://webgazer.cs.brown.edu/webgazer.js"></script>
    <style>
        body { font-family: 'Segoe UI', sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; background-color: #f8f9fa; }
        .container { background-color: white; padding: 40px; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }
        .instructions { background-color: #e7f3ff; border: 1px solid #b8daff; border-radius: 8px; padding: 10px 20px; margin-bottom: 30px; text-align: left; }
        .btn { background: #007bff; color: white; padding: 12px 30px; border: none; border-radius: 8px; font-size: 16px; cursor: pointer; margin: 10px; }
        .btn:disabled { background: #6c757d; }
        .hidden { display: none; }

        /* Calibration Styles */
        #calibration-section { text-align: center; }
        #calibration-feedback { position: relative; width: 100%; height: 400px; border: 2px solid #dee2e6; border-radius: 8px; margin: 30px 0; }
        .calibration-dot { width: 20px; height: 20px; border-radius: 50%; background-color: #dc3545; position: absolute; cursor: pointer; }

        /* Reading Task Styles */
        .abstract-display { display: flex; gap: 25px; margin: 25px 0; }
        .abstract-box { flex: 1; padding: 20px; border: 2px solid #dee2e6; border-radius: 8px; }
        .abstract-title { font-weight: bold; margin-bottom: 15px; color: #007bff; font-size: 18px; text-align: center; }
        .abstract-text { line-height: 1.8; text-align: justify; }
        .reading-timer { position: fixed; top: 10px; left: 10px; background-color: rgba(0,0,0,0.7); color: white; padding: 10px 15px; border-radius: 8px; z-index: 1000; }
        #webgazerVideoFeed { position: fixed; top: 10px; right: 10px; border: 2px solid #28a745; z-index: 1000; }
        .aoi { font-weight: bold; color: #d9534f; }
    </style>
</head>
<body>
    <div id="main-container" class="container">

        <div id="calibration-section">
            <h1>üîç Step 1: Eye Tracking Calibration</h1>
            <div class="instructions">
                <p><strong>Please follow these steps carefully:</strong></p>
                <ol>
                    <li>Click 'Allow' when your browser asks for camera permission.</li>
                    <li>A video of your face will appear. Make sure your eyes are clearly visible.</li>
                    <li>Click the red dot each time it appears (9 times). Keep your head still.</li>
                </ol>
            </div>
            <p id="status-message">Click the button below to start.</p>
            <button id="start-btn" class="btn" onclick="startCalibration()">Start Calibration</button>
            <div id="calibration-feedback" class="hidden"></div>
        </div>

        <div id="task1-section" class="hidden">
            <div class="reading-timer" id="reading-timer1">Reading Time: 0:00</div>
            <h1>üìñ Step 2: Reading Task (Part 1 of 2)</h1>
            <p>Please read both abstracts carefully. Your eye movements are being recorded. Click "Continue" when you are finished.</p>
            <div class="abstract-display">
                <div class="abstract-box"><div class="abstract-title">Abstract A</div><div class="abstract-text">This investigation <span class="aoi">sustains</span> that consciousness <span class="aoi">underlies</span> moral agency in decision-making processes. The research <span class="aoi">enables</span> autonomous choice through <span class="aoi">mediating</span> pathways that serve as the cornerstone of ethical reasoning frameworks. Statistical analysis <span class="aoi">reveals</span> significant predictors that <span class="aoi">translate</span> into <span class="aoi">measurable</span> behavioral outcomes across diverse populations.</div></div>
                <div class="abstract-box"><div class="abstract-title">Abstract B</div><div class="abstract-text">This investigation <span class="aoi">shows</span> that consciousness <span class="aoi">relates</span> to moral agency in decision-making processes. The research <span class="aoi">allows</span> autonomous choice through pathways that act as important elements of ethical reasoning frameworks. Statistical analysis <span class="aoi">shows</span> factors that lead to <span class="aoi">measurable</span> behavioral outcomes across diverse populations.</div></div>
            </div>
            <button class="btn" onclick="showNextTask()">Continue to Part 2</button>
        </div>

        <div id="task2-section" class="hidden">
            <div class="reading-timer" id="reading-timer2">Reading Time: 0:00</div>
            <h1>üìñ Step 3: Reading Task (Part 2 of 2)</h1>
            <p>Please read this second pair of abstracts carefully. Your eye movements are still being recorded. Click "Finish" when you are done.</p>
            <div class="abstract-display">
                <div class="abstract-box"><div class="abstract-title">Abstract A</div><div class="abstract-text">This study <span class="aoi">underlies</span> that ethical reasoning facilitates moral judgment in artificial intelligence systems. The analysis <span class="aoi">constrains</span> algorithmic bias through dialectical relationships that function as catalysts for technological change. Research <span class="aoi">explains</span> variance in discriminatory outcomes through <span class="aoi">mediating</span> variables that enable fair decision-making processes.</div></div>
                <div class="abstract-box"><div class="abstract-title">Abstract B</div><div class="abstract-text">This study <span class="aoi">relates</span> to ethical reasoning that <span class="aoi">supports</span> moral judgment in artificial intelligence systems. The analysis <span class="aoi">reduces</span> algorithmic bias through relationships that act as factors for technological change. Research <span class="aoi">shows</span> variation in discriminatory outcomes through variables that <span class="aoi">support</span> fair decision-making processes.</div></div>
            </div>
            <button class="btn" onclick="finishExperiment()">Finish Reading Task</button>
        </div>
    </div>

    <script src="js/eye-tracking-utils.js"></script>
    <script>
        // Global state variables
        let gazeData = [];
        let currentTask = 0; // 0=calibration, 1=task1, 2=task2
        let timerInterval;

        // --- CALIBRATION LOGIC ---
        let clickCounter = 0;
        const totalClicks = 9;

        async function startCalibration() {
            const startBtn = document.getElementById('start-btn');
            startBtn.disabled = true;
            updateStatus('Requesting camera access...');
            
            try {
                await webgazer.setRegression('ridge').setTracker('clmtrackr').begin();
                webgazer.showVideoPreview(true).showPredictionPoints(false);
                document.getElementById('calibration-feedback').classList.remove('hidden');
                startBtn.style.display = 'none';
                spawnDot();
            } catch (e) {
                console.error(e);
                updateStatus('Camera access denied. Please enable camera permissions and refresh the page.');
            }
        }

        function spawnDot() {
            updateStatus(`Click the red dot. (${clickCounter}/${totalClicks})`);
            const feedbackBox = document.getElementById('calibration-feedback');
            feedbackBox.innerHTML = '';
            const dot = document.createElement('div');
            dot.className = 'calibration-dot';
            dot.style.left = `${Math.random() * 95}%`;
            dot.style.top = `${Math.random() * 95}%`;
            
            dot.onclick = () => {
                webgazer.recordScreenPosition(dot.offsetLeft + 10, dot.offsetTop + 10, 'click');
                clickCounter++;
                if (clickCounter >= totalClicks) {
                    finishCalibration();
                } else {
                    spawnDot();
                }
            };
            feedbackBox.appendChild(dot);
        }

        function finishCalibration() {
            updateStatus('Calibration Complete!');
            document.getElementById('calibration-feedback').innerHTML = '<p>Great! The experiment will begin now.</p>';
            setTimeout(showNextTask, 2000); // Automatically start the first task after 2 seconds
        }

        function updateStatus(msg) {
            document.getElementById('status-message').innerText = msg;
        }


        // --- EXPERIMENT FLOW LOGIC ---
        function showNextTask() {
            if (currentTask === 0) { // From calibration to task 1
                currentTask = 1;
                document.getElementById('calibration-section').classList.add('hidden');
                document.getElementById('task1-section').classList.remove('hidden');
                startGazeRecording();
                startTimer('reading-timer1');
            } else if (currentTask === 1) { // From task 1 to task 2
                stopTimer();
                const dwellTime1 = window.eyeTracker.calculateDwellTimeOnAOIs(gazeData);
                sessionStorage.setItem('abstract1GazeData', JSON.stringify(gazeData));
                sessionStorage.setItem('abstract1DwellTime', JSON.stringify(dwellTime1));
                console.log(`Saved ${gazeData.length} gaze points for Task 1.`);
                console.log('Dwell Time for Task 1:', dwellTime1);
                
                gazeData = []; // Reset gaze data for the next task
                currentTask = 2;

                document.getElementById('task1-section').classList.add('hidden');
                document.getElementById('task2-section').classList.remove('hidden');
                startTimer('reading-timer2'); // Restart timer for the new task
            }
        }

        function startGazeRecording() {
            webgazer.setGazeListener((data, elapsedTime) => {
                if (data == null) return;
                gazeData.push({
                    x: data.x,
                    y: data.y,
                    time: elapsedTime,
                    task: currentTask // Tag data with the current task (1 or 2)
                });
            });
            console.log("Gaze recording is now active.");
        }

        function startTimer(timerId) {
            let startTime = Date.now();
            const timerElement = document.getElementById(timerId);
            timerInterval = setInterval(() => {
                const elapsed = Math.floor((Date.now() - startTime) / 1000);
                timerElement.innerText = `Reading Time: ${Math.floor(elapsed / 60)}m ${elapsed % 60}s`;
            }, 1000);
        }

        function stopTimer() {
            clearInterval(timerInterval);
        }

        function finishExperiment() {
            stopTimer();
            const dwellTime2 = window.eyeTracker.calculateDwellTimeOnAOIs(gazeData);
            sessionStorage.setItem('abstract2GazeData', JSON.stringify(gazeData));
            sessionStorage.setItem('abstract2DwellTime', JSON.stringify(dwellTime2));
            console.log(`Saved ${gazeData.length} gaze points for Task 2.`);
            console.log('Dwell Time for Task 2:', dwellTime2);

            // Stop WebGazer completely and navigate
            webgazer.end();
            window.location.href = 'results.html';
        }

    </script>
</body>
</html>
